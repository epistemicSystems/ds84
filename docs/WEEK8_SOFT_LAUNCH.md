# Week 8: Soft Launch & Iteration

## Overview

Week 8 focuses on production soft launch readiness with advanced monitoring, user analytics, feature flags, and iterative improvements. These capabilities enable safe, data-driven deployment and continuous optimization based on real-world usage.

---

## Table of Contents

1. [Performance Monitoring](#performance-monitoring)
2. [User Analytics & Telemetry](#user-analytics--telemetry)
3. [Feature Flags System](#feature-flags-system)
4. [API Endpoints](#api-endpoints)
5. [Production Readiness](#production-readiness)
6. [Monitoring Dashboards](#monitoring-dashboards)
7. [Best Practices](#best-practices)
8. [Troubleshooting](#troubleshooting)

---

## Performance Monitoring

### Real-Time Performance Tracking

The performance monitoring service automatically tracks all API requests with detailed metrics:

**Tracked Metrics:**
- Request count per endpoint
- Response time (min, max, avg, p50, p95, p99)
- Error count and error rate
- Status code distribution
- System resources (CPU, memory, disk)

### Automatic Monitoring

All requests are automatically tracked via middleware:

```python
# Automatic tracking for all endpoints
@app.middleware("http")
async def performance_tracking_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration_ms = (time.time() - start_time) * 1000

    performance_monitor.record_request(
        endpoint=request.url.path,
        method=request.method,
        duration_ms=duration_ms,
        status_code=response.status_code
    )

    return response
```

### Performance Endpoints

#### Get Performance Metrics
```bash
# All endpoints
GET /api/monitoring/performance

# Specific endpoint
GET /api/monitoring/performance?endpoint=/api/property-search
```

Response:
```json
{
  "POST:/api/property-search": {
    "endpoint": "/api/property-search",
    "method": "POST",
    "request_count": 1542,
    "error_count": 12,
    "error_rate": 0.0078,
    "duration_ms": {
      "min": 245.12,
      "max": 3421.56,
      "avg": 892.34,
      "p50": 823.45,
      "p95": 1654.23,
      "p99": 2341.78
    },
    "status_codes": {
      "200": 1520,
      "400": 10,
      "500": 12
    }
  }
}
```

#### Get Performance Summary
```bash
GET /api/monitoring/performance/summary
```

Response:
```json
{
  "uptime_seconds": 3600,
  "requests": {
    "total": 15234,
    "errors": 145,
    "error_rate": 0.0095
  },
  "performance": {
    "avg_duration_ms": 456.78,
    "p95_duration_ms": 1234.56,
    "p99_duration_ms": 2345.67
  },
  "endpoints": {
    "total": 42,
    "with_errors": 8
  },
  "system": {
    "cpu": {"percent": 45.2, "count": 4},
    "memory": {
      "total_mb": 8192,
      "used_mb": 3456,
      "percent": 42.2
    },
    "disk": {
      "total_gb": 100,
      "used_gb": 45,
      "percent": 45.0
    }
  }
}
```

#### Get Top Endpoints
```bash
# By request count
GET /api/monitoring/performance/top?limit=10&sort_by=request_count

# By error rate
GET /api/monitoring/performance/top?limit=10&sort_by=error_rate

# By latency
GET /api/monitoring/performance/top?limit=10&sort_by=avg_duration_ms
```

#### Get Slow Endpoints
```bash
# Endpoints with p95 > 1000ms
GET /api/monitoring/performance/slow?threshold_ms=1000&limit=10
```

### System Metrics

```bash
GET /api/monitoring/system
```

Real-time system resource monitoring:
- CPU usage and core count
- Memory usage (total, used, available)
- Disk usage
- Uptime

---

## User Analytics & Telemetry

### Event Tracking

Track user interactions and behavior:

**Event Types:**
- `api_request` - API calls
- `property_search` - Property searches
- `workflow_execution` - Workflow executions
- `ab_test_assignment` - A/B test assignments
- `page_view` - Page views
- `error` - Errors

### Tracking Events

```python
from app.services.analytics_service import analytics_service

# Track property search
analytics_service.track_property_search(
    user_id="user_123",
    query="3-bedroom house in San Jose",
    result_count=15,
    session_id="session_abc",
    duration_ms=456.78
)

# Track workflow execution
analytics_service.track_workflow_execution(
    user_id="user_123",
    workflow_id="property_search",
    execution_id="exec_xyz",
    status="completed",
    duration_ms=1234.56
)

# Track error
analytics_service.track_error(
    user_id="user_123",
    error_type="ValidationError",
    error_message="Invalid input",
    context={"field": "price"},
    session_id="session_abc"
)
```

### Analytics Endpoints

#### Get User Analytics
```bash
GET /api/analytics/user/user_123?days=30
```

Response:
```json
{
  "user_id": "user_123",
  "total_events": 156,
  "event_types": {
    "property_search": 45,
    "page_view": 89,
    "workflow_execution": 22
  },
  "sessions": 12,
  "first_seen": "2025-10-01T10:00:00Z",
  "last_seen": "2025-10-22T15:30:00Z",
  "days_active": 30,
  "avg_events_per_session": 13.0
}
```

#### Get Feature Usage
```bash
GET /api/analytics/features?limit=20
```

Shows most used features:
```json
{
  "features": [
    {"feature": "property_search", "usage_count": 5432},
    {"feature": "workflow_execution", "usage_count": 3421},
    {"feature": "ab_test_assignment", "usage_count": 1234}
  ]
}
```

#### Get Active Users
```bash
# Active in last 60 minutes
GET /api/analytics/active-users?minutes=60
```

Response:
```json
{
  "time_window_minutes": 60,
  "active_users": 45,
  "active_sessions": 52,
  "timestamp": "2025-10-22T15:30:00Z"
}
```

#### Get Session Analytics
```bash
GET /api/analytics/session/session_abc
```

Detailed session information:
```json
{
  "session_id": "session_abc",
  "found": true,
  "user_id": "user_123",
  "start_time": "2025-10-22T14:00:00Z",
  "last_activity": "2025-10-22T15:30:00Z",
  "duration_seconds": 5400,
  "event_count": 24,
  "event_types": {
    "property_search": 8,
    "page_view": 12,
    "workflow_execution": 4
  },
  "events": ["page_view", "property_search", ...]
}
```

#### Get Error Summary
```bash
GET /api/analytics/errors?hours=24
```

Error tracking and analysis:
```json
{
  "time_window_hours": 24,
  "total_errors": 45,
  "unique_users_affected": 23,
  "error_types": {
    "ValidationError": 25,
    "TimeoutError": 12,
    "DatabaseError": 8
  },
  "recent_errors": [...]
}
```

#### Conversion Funnel Analysis
```bash
POST /api/analytics/funnel
Content-Type: application/json

{
  "funnel_steps": [
    "page_view",
    "property_search",
    "property_view",
    "contact_agent"
  ],
  "days": 7
}
```

Response:
```json
{
  "funnel_steps": ["page_view", "property_search", "property_view", "contact_agent"],
  "time_window_days": 7,
  "analysis": [
    {
      "step": 1,
      "event_type": "page_view",
      "users": 1000,
      "conversion_rate": 1.0,
      "dropoff_rate": 0.0
    },
    {
      "step": 2,
      "event_type": "property_search",
      "users": 750,
      "conversion_rate": 0.75,
      "dropoff_rate": 0.25
    },
    {
      "step": 3,
      "event_type": "property_view",
      "users": 450,
      "conversion_rate": 0.6,
      "dropoff_rate": 0.4
    },
    {
      "step": 4,
      "event_type": "contact_agent",
      "users": 120,
      "conversion_rate": 0.267,
      "dropoff_rate": 0.733
    }
  ],
  "overall_conversion": 0.12
}
```

---

## Feature Flags System

### Controlled Feature Rollouts

Feature flags enable safe, gradual feature rollouts with multiple strategies:

**Rollout Strategies:**
1. **All Users** - Feature enabled for everyone
2. **Percentage** - Gradual rollout to percentage of users
3. **User List** - Specific users (whitelist/blacklist)
4. **User Attributes** - Target by user attributes (e.g., plan type)
5. **Gradual** - Incrementally increase percentage

### Default Feature Flags

Pre-configured flags for all major features:

```python
# A/B Testing
feature_flags.is_enabled("ab_testing", user_id)  # True

# Feedback Learning
feature_flags.is_enabled("feedback_learning", user_id)  # True

# Meta-cognitive Optimization
feature_flags.is_enabled("meta_cognitive", user_id)  # True

# Experimental Features (5% rollout)
feature_flags.is_enabled("experimental_features", user_id)  # True for 5% of users

# GPT-4 for All Workflows (10% rollout)
feature_flags.is_enabled("gpt4_all_workflows", user_id)  # True for 10% of users
```

### Creating Feature Flags

```python
from app.services.feature_flags import feature_flags, RolloutStrategy

# Create percentage-based rollout
feature_flags.create_flag(
    flag_id="new_search_ui",
    name="New Search UI",
    description="Redesigned property search interface",
    enabled=True,
    strategy=RolloutStrategy.PERCENTAGE,
    percentage=25.0  # 25% of users
)

# Create attribute-based targeting
feature_flags.create_flag(
    flag_id="premium_features",
    name="Premium Features",
    description="Features for premium users",
    enabled=True,
    strategy=RolloutStrategy.USER_ATTRIBUTE,
    attributes={"plan": "premium"}
)
```

### Checking Feature Flags

```python
# Simple check
enabled = feature_flags.is_enabled("new_search_ui", user_id="user_123")

# With user attributes
enabled = feature_flags.is_enabled(
    "premium_features",
    user_id="user_123",
    user_attributes={"plan": "premium", "region": "US"}
)

# Get all flags for user
user_flags = feature_flags.get_user_flags(
    user_id="user_123",
    user_attributes={"plan": "premium"}
)
# Returns: {"ab_testing": True, "experimental_features": False, ...}
```

### Feature Flag Endpoints

#### List All Flags
```bash
# All flags
GET /api/feature-flags

# Only enabled flags
GET /api/feature-flags?enabled_only=true
```

#### Get Specific Flag
```bash
GET /api/feature-flags/experimental_features
```

Response:
```json
{
  "flag_id": "experimental_features",
  "name": "Experimental Features",
  "description": "Enable experimental and beta features",
  "enabled": true,
  "strategy": "percentage",
  "percentage": 5.0,
  "user_whitelist_count": 10,
  "user_blacklist_count": 0,
  "created_at": "2025-10-01T10:00:00Z",
  "updated_at": "2025-10-15T14:30:00Z"
}
```

#### Check if Enabled for User
```bash
POST /api/feature-flags/experimental_features/check
Content-Type: application/json

{
  "user_id": "user_123",
  "user_attributes": {"plan": "free"}
}
```

Response:
```json
{
  "flag_id": "experimental_features",
  "user_id": "user_123",
  "enabled": true
}
```

#### Get All Flags for User
```bash
GET /api/feature-flags/user/user_123
```

Returns all flags and their status for the user.

#### Get Flag Statistics
```bash
GET /api/feature-flags/stats
```

Overview of all feature flags:
```json
{
  "total_flags": 8,
  "enabled_flags": 6,
  "disabled_flags": 2,
  "strategies": {
    "all_users": 4,
    "percentage": 2,
    "user_list": 1,
    "user_attribute": 1
  },
  "flags": [...]
}
```

### Gradual Rollout

Safely increase feature adoption:

```python
# Plan gradual rollout from 5% to 100%
plan = feature_flags.gradual_rollout(
    flag_id="new_search_ui",
    target_percentage=100.0,
    increment=10.0  # Increase by 10% each step
)

# Returns:
# {
#   "current_percentage": 5.0,
#   "target_percentage": 100.0,
#   "steps": [15.0, 25.0, 35.0, ..., 95.0, 100.0],
#   "total_steps": 10,
#   "recommendation": "Gradually increase from 5% to 100% in 10 steps"
# }

# Then manually increase percentage based on monitoring:
feature_flags.update_flag("new_search_ui", percentage=15.0)
# Monitor for issues, then continue
feature_flags.update_flag("new_search_ui", percentage=25.0)
# etc.
```

---

## API Endpoints Summary

### Monitoring Endpoints (18 total)

**Performance Monitoring:**
- `GET /api/monitoring/performance` - All endpoint metrics
- `GET /api/monitoring/performance/summary` - Overall summary
- `GET /api/monitoring/performance/top` - Top endpoints by metric
- `GET /api/monitoring/performance/slow` - Slow endpoints
- `GET /api/monitoring/system` - System resource metrics

**Analytics:**
- `GET /api/analytics/user/{user_id}` - User analytics
- `GET /api/analytics/features` - Feature usage stats
- `GET /api/analytics/active-users` - Active users count
- `GET /api/analytics/session/{session_id}` - Session analytics
- `GET /api/analytics/errors` - Error summary
- `POST /api/analytics/funnel` - Conversion funnel analysis

**Feature Flags:**
- `GET /api/feature-flags` - List all flags
- `GET /api/feature-flags/{flag_id}` - Get specific flag
- `POST /api/feature-flags/{flag_id}/check` - Check if enabled for user
- `GET /api/feature-flags/user/{user_id}` - Get all flags for user
- `GET /api/feature-flags/stats` - Flag statistics

---

## Production Readiness

### Pre-Launch Checklist

#### Infrastructure
- âœ… Docker containerization
- âœ… CI/CD pipeline
- âœ… Health checks (liveness, readiness)
- âœ… Structured logging
- âœ… Monitoring and metrics
- âœ… Performance tracking
- âœ… Error tracking

#### Security
- âœ… API key authentication
- âœ… Rate limiting
- âœ… Input validation
- âœ… SQL injection protection
- âœ… XSS protection
- âœ… Command injection protection

#### Functionality
- âœ… Prompt chaining
- âœ… Vector search
- âœ… Cognitive workflows
- âœ… Meta-cognitive optimization
- âœ… Feedback learning
- âœ… A/B testing
- âœ… Caching

#### Monitoring
- âœ… Real-time performance metrics
- âœ… User analytics
- âœ… Error tracking
- âœ… Feature flags
- âœ… System resource monitoring

### Launch Strategy

**Phase 1: Internal Testing (Week 1)**
- Deploy to staging environment
- Internal team testing
- Performance baseline establishment
- Feature flag testing (0-5% rollout)

**Phase 2: Limited Beta (Week 2-3)**
- Invite 50-100 beta users
- Enable experimental_features for beta users
- Monitor performance and errors closely
- Collect user feedback
- Gradual rollout: 5% â†’ 10% â†’ 25%

**Phase 3: Expanded Beta (Week 4-5)**
- Expand to 500-1000 users
- Gradually enable more features
- A/B test optimizations
- Refine based on analytics
- Rollout: 25% â†’ 50% â†’ 75%

**Phase 4: General Availability (Week 6+)**
- Public launch
- Full feature availability
- Continuous monitoring and optimization
- Rollout: 75% â†’ 100%

### Monitoring During Launch

**Key Metrics to Watch:**
1. **Performance**: p95 latency < 1s, error rate < 1%
2. **Availability**: Uptime > 99.9%
3. **User Engagement**: DAU, session duration
4. **Errors**: Track all errors, fix critical issues within 24h
5. **Resource Usage**: CPU < 70%, Memory < 80%

**Alerts to Configure:**
- Error rate > 5% (critical)
- p95 latency > 2s (warning)
- CPU usage > 80% (warning)
- Memory usage > 90% (critical)
- Failed health checks (critical)

---

## Monitoring Dashboards

### Recommended Dashboards

#### 1. System Health Dashboard
- Uptime and availability
- Request rate (req/s)
- Error rate
- P50, P95, P99 latency
- CPU, memory, disk usage

#### 2. User Analytics Dashboard
- Active users (5m, 1h, 24h)
- New users vs. returning
- Sessions per user
- Average session duration
- Feature usage breakdown

#### 3. Performance Dashboard
- Slowest endpoints
- Highest traffic endpoints
- Error-prone endpoints
- Response time distribution
- Cache hit rates

#### 4. Business Metrics Dashboard
- Property searches per day
- Workflow executions
- User engagement funnel
- Feature adoption rates
- A/B test results

### Grafana Dashboard Example

```yaml
# Import Prometheus metrics
datasource: prometheus

# Panels:
- Request Rate: rate(http_requests_total[5m])
- Error Rate: rate(http_requests_total{status=~"5.."}[5m])
- P95 Latency: histogram_quantile(0.95, http_request_duration_seconds)
- Active Users: active_users_total
```

---

## Best Practices

### Performance Monitoring
1. **Establish Baselines**: Record normal performance metrics before launch
2. **Set Thresholds**: Define acceptable latency and error rates
3. **Monitor Trends**: Watch for gradual degradation
4. **Regular Reviews**: Weekly performance review meetings
5. **Automated Alerts**: Don't rely on manual monitoring

### Analytics
1. **Privacy First**: Anonymize user data where possible
2. **Data Retention**: Define retention policies (e.g., 90 days)
3. **Actionable Metrics**: Focus on metrics that drive decisions
4. **Segment Users**: Analyze by cohorts, regions, plans
5. **Correlation Not Causation**: Careful with metric interpretation

### Feature Flags
1. **Start Small**: Begin with low percentages (1-5%)
2. **Monitor Closely**: Watch metrics after each increase
3. **Have Rollback Plan**: Be ready to disable features quickly
4. **Document Changes**: Log all flag updates with reasoning
5. **Clean Up**: Remove flags after full rollout

### Incident Response
1. **Triage Quickly**: Assess severity within 5 minutes
2. **Communicate**: Update status page, notify users
3. **Rollback First**: If unsure, rollback to stable version
4. **Root Cause**: Conduct blameless postmortems
5. **Prevent Recurrence**: Update monitoring, add safeguards

---

## Troubleshooting

### High Latency

**Symptoms:**
- P95 latency > 2s
- Slow response times reported by users

**Investigation:**
```bash
# Check slow endpoints
GET /api/monitoring/performance/slow?threshold_ms=1000

# Check system resources
GET /api/monitoring/system

# Review cache stats
GET /health/detailed
```

**Common Causes:**
- Database query optimization needed
- Cache misses
- High CPU/memory usage
- External API slowness

**Solutions:**
- Increase cache sizes
- Optimize database queries
- Scale horizontally (add workers)
- Add request timeouts

### High Error Rate

**Symptoms:**
- Error rate > 5%
- Increase in 500 errors

**Investigation:**
```bash
# Get error summary
GET /api/analytics/errors?hours=1

# Check error-prone endpoints
GET /api/monitoring/performance/top?sort_by=error_rate

# Review recent errors
Check logs: tail -f logs/app.log | jq 'select(.level == "ERROR")'
```

**Common Causes:**
- Database connection issues
- OpenAI API rate limits
- Invalid input not caught by validation
- Memory exhaustion

**Solutions:**
- Add retry logic
- Increase rate limits
- Improve input validation
- Add circuit breakers

### Low User Engagement

**Symptoms:**
- Low DAU/MAU ratio
- Short session durations
- High funnel dropoff

**Investigation:**
```bash
# Analyze funnel
POST /api/analytics/funnel
{
  "funnel_steps": ["page_view", "property_search", "property_view"],
  "days": 7
}

# Check feature usage
GET /api/analytics/features

# Review session analytics
GET /api/analytics/session/{session_id}
```

**Common Causes:**
- Poor UX
- Performance issues
- Lack of valuable features
- Confusing onboarding

**Solutions:**
- A/B test UX improvements
- Optimize performance
- Add requested features
- Improve onboarding

---

## Summary

Week 8 delivers production-ready soft launch capabilities:

âœ… **Performance Monitoring**: Real-time metrics for all endpoints
âœ… **User Analytics**: Comprehensive user behavior tracking
âœ… **Feature Flags**: Safe, controlled feature rollouts
âœ… **Error Tracking**: Detailed error monitoring and analysis
âœ… **Conversion Funnels**: Understand user journey and dropoff
âœ… **System Metrics**: CPU, memory, disk monitoring
âœ… **18 New API Endpoints**: Complete monitoring and analytics API

**The REALTOR AI COPILOT is now ready for production soft launch with:**
- Real-time performance visibility
- Data-driven decision making
- Safe feature rollouts
- Comprehensive user analytics
- Proactive error detection
- Continuous optimization capabilities

ðŸš€ **Ready to launch and iterate based on real-world data!**
